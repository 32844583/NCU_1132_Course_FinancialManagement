{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11940123,"sourceType":"datasetVersion","datasetId":7506512}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok --quiet\n!pip install flask --quiet\n!pip install faiss-gpu-cu12 --quiet # CUDA 12.x, Python 3.8+\n!pip install langchain-community --quiet\n!pip install sentence-transformers --quiet\n!pip install transformers torch --quiet\n!pip install joblib --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:44:33.943038Z","iopub.execute_input":"2025-06-16T03:44:33.943297Z","iopub.status.idle":"2025-06-16T03:46:14.989731Z","shell.execute_reply.started":"2025-06-16T03:44:33.943275Z","shell.execute_reply":"2025-06-16T03:46:14.988853Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pyngrok import ngrok, conf\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nNGROK_AUTH_TOKEN_FROM_SECRET = None\n\ntry:\n    user_secrets = UserSecretsClient()\n    # 假設您在 Kaggle Secret \"NGROK_API_KEY\" 中儲存的是 Authtoken\n    NGROK_AUTH_TOKEN_FROM_SECRET = user_secrets.get_secret(\"NGROK_A_TOKEN\")\n    print('NGROK_AUTH_TOKEN_FROM_SECRET:', NGROK_AUTH_TOKEN_FROM_SECRET)\n    print(\"成功從 Kaggle Secrets 讀取 NGROK Authtoken。\")\n\n    if NGROK_AUTH_TOKEN_FROM_SECRET:\n        ngrok.set_auth_token(NGROK_AUTH_TOKEN_FROM_SECRET)\n        print(\"Ngrok Authtoken 已成功配置到 pyngrok。\")\n    else:\n        print(\"從 Kaggle Secrets 讀取的 Authtoken 為空。\")\n\n\nexcept Exception as e:\n    print(f\"從 Kaggle Secrets 讀取 'NGROK_API_KEY' (用於 Authtoken) 失敗: {e}\")\n    print(\"Ngrok 將以未驗證模式運行，可能會有一些限制。\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:46:14.991210Z","iopub.execute_input":"2025-06-16T03:46:14.991470Z","iopub.status.idle":"2025-06-16T03:46:16.588314Z","shell.execute_reply.started":"2025-06-16T03:46:14.991442Z","shell.execute_reply":"2025-06-16T03:46:16.587598Z"}},"outputs":[{"name":"stdout","text":"NGROK_AUTH_TOKEN_FROM_SECRET: 2xY6WUm0ED0YHABm3QgnURYWSya_4HVHpCKquvBG8KsVX83cz\n成功從 Kaggle Secrets 讀取 NGROK Authtoken。\nNgrok Authtoken 已成功配置到 pyngrok。                                                                     \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from flask import Flask, request, jsonify # 新增 request, jsonify\nimport threading\nimport time\n\nFLASK_PORT = 5002 # Flask 應用程式監聽的端口\npublic_url_ngrok = None # 用來存放 ngrok 的公開 URL\n\n# --- 啟動 Flask 應用程式的函數 (在單獨的執行緒中) ---\ndef run_flask_app():\n    print(f\"啟動 Flask 應用程式，監聽端口 {FLASK_PORT}...\")\n    app.run(host='0.0.0.0', port=FLASK_PORT, debug=False, use_reloader=False) #\n\n# --- 啟動 Ngrok 通道的函數 ---\ndef start_ngrok_tunnel():\n    global public_url_ngrok\n    try:\n        public_url_ngrok = ngrok.connect(FLASK_PORT) #\n        print(f\"Ngrok 通道已開啟。\")\n        print(f\" * 公開 URL: {public_url_ngrok}\") #\n        print(f\" * 本地 URL: http://127.0.0.1:{FLASK_PORT}\") #\n    except Exception as e:\n        print(f\"開啟 Ngrok 通道時發生錯誤: {e}\") #\n        tunnels = ngrok.get_tunnels() #\n        if tunnels:\n            print(\"偵測到已存在的 Ngrok 通道:\") #\n            public_url_ngrok = tunnels[0].public_url #\n            print(f\"將使用已存在的通道: {public_url_ngrok}\") #\n        else:\n            print(\"無法建立或找到 Ngrok 通道。\") #\n\nprint(\"Flask app 和 Ngrok 輔助函式已定義。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:46:16.589204Z","iopub.execute_input":"2025-06-16T03:46:16.589462Z","iopub.status.idle":"2025-06-16T03:46:16.880000Z","shell.execute_reply.started":"2025-06-16T03:46:16.589443Z","shell.execute_reply":"2025-06-16T03:46:16.879319Z"}},"outputs":[{"name":"stdout","text":"Flask app 和 Ngrok 輔助函式已定義。\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\") # \"HF_TOKEN\" 是您在 Secrets 中設定的名稱\n    login(token=hf_token)\n    print(\"Hugging Face Hub login successful using Kaggle Secrets.\")\nexcept Exception as e:\n    print(f\"Hugging Face Hub login failed using Kaggle Secrets: {e}\")\n    print(\"Please ensure you have added your Hugging Face token as a secret named 'HF_TOKEN' in your Kaggle Notebook.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:46:16.881694Z","iopub.execute_input":"2025-06-16T03:46:16.881947Z","iopub.status.idle":"2025-06-16T03:46:17.521320Z","shell.execute_reply.started":"2025-06-16T03:46:16.881928Z","shell.execute_reply":"2025-06-16T03:46:17.520571Z"}},"outputs":[{"name":"stdout","text":"Hugging Face Hub login successful using Kaggle Secrets.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import faiss\nimport pickle\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline as hf_pipeline\nimport torch\nfrom langchain_community.llms import HuggingFacePipeline # 或您使用的其他 LLM 封裝\n\n# --- RAG 元件全域變數 ---\nfaiss_index_global = None\nall_chunks_global = None\nembedding_model_global = None\nllm_pipeline_global = None # 我們將直接使用 Langchain Pipeline\n# tokenizer_global 和 llm_model_global 僅在設定 llm_pipeline_global 時臨時使用\n\nRAG_DATA_PATH = \"/kaggle/input/rag-files/\"\n\ndef load_rag_components_sequential():\n    global faiss_index_global, all_chunks_global, embedding_model_global, llm_pipeline_global\n    print(\"開始依序載入 RAG 元件...\")\n\n    try:\n        print(f\"\\n[1/4] 正在從 '{os.path.join(RAG_DATA_PATH, 'my_company_index.faiss')}' 載入 FAISS 索引...\")\n        faiss_index_global = faiss.read_index(os.path.join(RAG_DATA_PATH, \"my_company_index.faiss\"))\n        print(f\"  FAISS 索引已載入，包含 {faiss_index_global.ntotal} 個向量。\")\n    except Exception as e:\n        print(f\"  載入 FAISS 索引失敗: {e}\")\n        return False # 關鍵元件載入失敗，終止\n\n    try:\n        print(f\"\\n[2/4] 正在從 '{os.path.join(RAG_DATA_PATH, 'all_chunks.pkl')}' 載入文本區塊 (all_chunks)...\")\n        with open(os.path.join(RAG_DATA_PATH, 'all_chunks.pkl'), 'rb') as f:\n            all_chunks_global = pickle.load(f)\n        print(f\"  All_chunks 已載入，共 {len(all_chunks_global)} 個區塊。\")\n    except Exception as e:\n        print(f\"  載入 all_chunks 失敗: {e}\")\n        return False\n\n    try:\n        print(\"\\n[3/4] 正在載入嵌入模型 (infgrad/stella-large-zh-v2)...\")\n        # 為了節省 Kaggle 環境的重複下載，您可以先將模型下載到輸出目錄，再從輸出目錄作為輸入來載入\n        # embedding_model_path = os.path.join(RAG_DATA_PATH, \"stella-large-zh-v2-model/\") # 如果您已手動下載\n        # embedding_model_global = SentenceTransformer(embedding_model_path)\n        embedding_model_global = SentenceTransformer('infgrad/stella-large-zh-v2')\n        print(\"  嵌入模型已載入。\")\n    except Exception as e:\n        print(f\"  載入嵌入模型失敗: {e}\")\n        return False\n\n    try:\n        print(\"\\n[4/4] 正在載入 LLM 並設定 Pipeline (taide/Llama3-TAIDE-LX-8B-Chat-Alpha1)...\")\n        model_id_llm = \"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\" \n        print(f\"  將嘗試載入小型 LLM: {model_id_llm} 以進行測試。\")\n\n        local_llm_tokenizer = AutoTokenizer.from_pretrained(model_id_llm)\n        local_llm_model = AutoModelForCausalLM.from_pretrained(\n            model_id_llm,\n            device_map=\"auto\",\n            # load_in_8bit=True, # 使用 8-bit 量化\n            torch_dtype=torch.float16, # 與 load_in_8bit 通常不同時使用，或根據 bitsandbytes 文件調整\n        )\n        print(f\"Model device map: {local_llm_model.hf_device_map if hasattr(local_llm_model, 'hf_device_map') else 'N/A'}\")\n        pipe = hf_pipeline(\n            \"text-generation\",\n            model=local_llm_model,\n            tokenizer=local_llm_tokenizer,\n            max_new_tokens=512, # 限制生成長度\n            # temperature=0.7, # 可選\n            # top_p=0.95      # 可選\n        )\n        llm_pipeline_global = HuggingFacePipeline(pipeline=pipe)\n        print(f\"  LLM ({model_id_llm}) Pipeline 已設定。\")\n    except Exception as e:\n        print(f\"  載入 LLM 或設定 Pipeline 失敗: {e}\")\n        print(\"  提示：如果出現記憶體不足 (OOM) 的錯誤，請嘗試更小的 LLM 模型，或在具有更多 RAM 的環境中執行。\")\n        return False\n\n    print(\"\\n所有 RAG 元件已成功載入並設定。\")\n    return True\n\nprint(\"RAG 元件載入函式已定義。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:46:17.522191Z","iopub.execute_input":"2025-06-16T03:46:17.522429Z","iopub.status.idle":"2025-06-16T03:46:44.231612Z","shell.execute_reply.started":"2025-06-16T03:46:17.522412Z","shell.execute_reply":"2025-06-16T03:46:44.230855Z"}},"outputs":[{"name":"stderr","text":"2025-06-16 03:46:29.204343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750045589.400088      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750045589.455836      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"RAG 元件載入函式已定義。\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 確保 RAG_DATA_PATH 在上一個 cell 中已正確設定\nif os.path.exists(RAG_DATA_PATH):\n    print(f\"將從路徑 '{RAG_DATA_PATH}' 載入 RAG 檔案。\")\n    rag_components_loaded_successfully = load_rag_components_sequential()\n    if rag_components_loaded_successfully:\n        print(\"\\n✅ RAG 元件載入測試成功！\")\n    else:\n        print(\"\\n❌ RAG 元件載入測試失敗。請檢查上述錯誤訊息和檔案路徑。\")\nelse:\n    print(f\"❌ 錯誤：指定的 RAG_DATA_PATH '{RAG_DATA_PATH}' 不存在。請檢查路徑設定。\")\n    rag_components_loaded_successfully = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:46:44.232444Z","iopub.execute_input":"2025-06-16T03:46:44.233081Z","iopub.status.idle":"2025-06-16T03:48:26.587726Z","shell.execute_reply.started":"2025-06-16T03:46:44.233055Z","shell.execute_reply":"2025-06-16T03:48:26.586922Z"}},"outputs":[{"name":"stdout","text":"將從路徑 '/kaggle/input/rag-files/' 載入 RAG 檔案。\n開始依序載入 RAG 元件...\n\n[1/4] 正在從 '/kaggle/input/rag-files/my_company_index.faiss' 載入 FAISS 索引...\n  FAISS 索引已載入，包含 43 個向量。\n\n[2/4] 正在從 '/kaggle/input/rag-files/all_chunks.pkl' 載入文本區塊 (all_chunks)...\n  All_chunks 已載入，共 43 個區塊。\n\n[3/4] 正在載入嵌入模型 (infgrad/stella-large-zh-v2)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1393734bd54403ba231d3ce703f97cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/652M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6596e519e80b4cda8cbac8c206f9d7f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/652M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6331ce5ea7947318532fe6e347f99e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0a92f89dac4a798685690d1300e2ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed2ada8c75e4f548423fbb75f5671c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/439k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cff8de708d04ac0b4ffaf849bfdb777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f56bb80bb8c4b39803aed1d7a7a3c01"}},"metadata":{}},{"name":"stdout","text":"  嵌入模型已載入。\n\n[4/4] 正在載入 LLM 並設定 Pipeline (taide/Llama3-TAIDE-LX-8B-Chat-Alpha1)...\n  將嘗試載入小型 LLM: taide/Llama3-TAIDE-LX-8B-Chat-Alpha1 以進行測試。\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8015f999b8e4ddf84c8ed8124a58b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f450dc230345b0abd5f4750673b874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150284d6c8614107861f0629e04aa3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ab057b5bd9e471090e5d3086b4bdcc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf6d1c153534e69816fd035b80c6789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d05fa0b9e44764b9d5275dd9f6bc05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b12a39ef0f4294af2bd5748097fcec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"199311269d7247a9bfad11a2aa86f23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555ad2a4f5b7448ea29ec7b7576eadee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c06be2a30b4525b0db0b49a4c08dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4e70f7e439410ba3afeb8c8fdcee31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/143 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e56115b6e44b80ab30570f79657f7e"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n  LLM (taide/Llama3-TAIDE-LX-8B-Chat-Alpha1) Pipeline 已設定。\n\n所有 RAG 元件已成功載入並設定。\n\n✅ RAG 元件載入測試成功！\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/880638246.py:71: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm_pipeline_global = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# 這個 prompt 模板也應該和您 RAG 筆記本中的一致\n# 為了適配小型模型，prompt 可能需要更簡潔\nRAG_PROMPT_TEMPLATE_STRING =\"\"\"\n基於以下上下文回答問題。如果上下文中沒有答案，請說「我不知道」。\n上下文：{context}\n問題：{question}\n答案：\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:48:26.588652Z","iopub.execute_input":"2025-06-16T03:48:26.589308Z","iopub.status.idle":"2025-06-16T03:48:26.708802Z","shell.execute_reply.started":"2025-06-16T03:48:26.589279Z","shell.execute_reply":"2025-06-16T03:48:26.708335Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def process_query_with_rag(user_question_str: str):\n    global faiss_index_global, all_chunks_global, embedding_model_global, llm_pipeline_global\n\n    if not all([faiss_index_global, all_chunks_global, embedding_model_global, llm_pipeline_global]):\n        print(\"錯誤：RAG 元件未完全初始化。\")\n        return \"RAG 服務錯誤：元件未初始化。\", []\n\n    print(f\"\\n[RAG] 接收到問題: {user_question_str}\")\n\n    # 1. 嵌入問題\n    print(\"[RAG] 步驟 1: 嵌入問題...\")\n    query_embedding = embedding_model_global.encode(user_question_str)\n    query_embedding_final = query_embedding.reshape(1, -1).astype(np.float32)\n    print(\"[RAG]   問題嵌入完成。\")\n\n    # 2. FAISS 搜尋\n    print(\"[RAG] 步驟 2: 在 FAISS 索引中搜尋相似區塊 (k=3)...\")\n    k = 3 # 檢索數量\n    if faiss_index_global.ntotal == 0:\n        print(\"[RAG]   錯誤：FAISS 索引是空的。\")\n        return \"RAG 服務錯誤：知識庫為空。\", []\n    distances, indices = faiss_index_global.search(query_embedding_final, k)\n    print(f\"[RAG]   搜尋完成。找到索引: {indices}\")\n\n    # 3. 檢索上下文\n    print(\"[RAG] 步驟 3: 提取上下文區塊...\")\n    retrieved_context_chunks_content = [\n        all_chunks_global[idx] for idx in indices[0] if 0 <= idx < len(all_chunks_global)\n    ]\n    if not retrieved_context_chunks_content:\n        print(\"[RAG]   未能檢索到任何有效的上下文區塊。\")\n        relevant_context_str_for_llm = \"找不到相關上下文。\"\n    else:\n        print(f\"[RAG]   成功提取 {len(retrieved_context_chunks_content)} 個上下文區塊。\")\n        print(\"[RAG]   --- 檢索到的區塊具體內容 ---\")\n        for i, chunk_content in enumerate(retrieved_context_chunks_content):\n            # 使用 .strip() 去除前後多餘的空白或換行符\n            # 為了避免輸出過長，可以只顯示部分內容，例如前 200 字\n            print(f\"[RAG]   [區塊 {i+1}]: {chunk_content.strip()[:200]}...\")\n        print(\"[RAG]   -----------------------------\")\n\n        relevant_context_str_for_llm = \" \".join(\n            chunk.replace(\"\\n\", \" \") for chunk in retrieved_context_chunks_content\n        )\n        \n    # 4. 使用 LLM 生成答案\n    print(\"[RAG] 步驟 4: 使用 LLM 生成答案...\")\n    prompt_template = PromptTemplate(\n        template=RAG_PROMPT_TEMPLATE_STRING,\n        input_variables=[\"context\", \"question\"]\n    )\n    llm_chain = LLMChain(llm=llm_pipeline_global, prompt=prompt_template)\n    \n    try:\n        chain_input_payload = {\"question\": user_question_str, \"context\": relevant_context_str_for_llm}\n        chain_output_payload = llm_chain.invoke(chain_input_payload)\n        final_answer_from_llm = chain_output_payload.get(\"text\", \"LLM 未能生成有效答案。\")\n        # 對於某些 pipeline 輸出，可能需要進一步解析來提取純文本答案\n        # 例如，如果答案包含原始輸入，您可能需要去除它\n        # TinyLlama 的輸出通常是完整的，包括 prompt，需要處理\n        # 嘗試找到 \"答案：\" 之後的內容\n        answer_marker = \"答案：\"\n        if answer_marker in final_answer_from_llm:\n            final_answer_from_llm = final_answer_from_llm.split(answer_marker, 1)[-1].strip()\n\n        print(f\"[RAG]   LLM 生成的原始答案 (部分): {final_answer_from_llm[:200]}...\")\n    except Exception as e_llm:\n        print(f\"[RAG]   LLM Chain 執行錯誤: {e_llm}\")\n        final_answer_from_llm = f\"LLM 處理時發生錯誤: {e_llm}\"\n    \n    return final_answer_from_llm, retrieved_context_chunks_content\n\nprint(\"RAG 查詢處理函式 'process_query_with_rag' 已定義。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:48:26.709556Z","iopub.execute_input":"2025-06-16T03:48:26.709794Z","iopub.status.idle":"2025-06-16T03:48:26.719039Z","shell.execute_reply.started":"2025-06-16T03:48:26.709770Z","shell.execute_reply":"2025-06-16T03:48:26.718298Z"}},"outputs":[{"name":"stdout","text":"RAG 查詢處理函式 'process_query_with_rag' 已定義。\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"if rag_components_loaded_successfully:\n    print(\"準備測試 RAG 查詢函式...\")\n    test_question = \"聯發科的主要業務是什麼？\" # 換成一個您認為資料庫中可能有的問題\n    # test_question = \"TSMC Q1 revenue\" # 如果您用英文資料和模型\n    print(f\"測試問題: {test_question}\")\n\n    answer, context = process_query_with_rag(test_question)\n\n    print(\"\\n--- RAG 函式測試結果 ---\")\n    print(f\"問題: {test_question}\")\n    print(f\"答案: {answer}\")\n    # print(\"\\n檢索到的部分上下文:\")\n    # for i, ctx_chunk in enumerate(context[:1]): # 只印第一個上下文避免過長\n    #     print(f\"--- 上下文 {i+1} ---\\n{ctx_chunk[:300]}...\\n\")\nelse:\n    print(\"由於 RAG 元件未成功載入，跳過 RAG 查詢函式測試。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:48:26.719655Z","iopub.execute_input":"2025-06-16T03:48:26.719844Z","iopub.status.idle":"2025-06-16T03:49:10.968501Z","shell.execute_reply.started":"2025-06-16T03:48:26.719829Z","shell.execute_reply":"2025-06-16T03:49:10.967800Z"}},"outputs":[{"name":"stdout","text":"準備測試 RAG 查詢函式...\n測試問題: 聯發科的主要業務是什麼？\n\n[RAG] 接收到問題: 聯發科的主要業務是什麼？\n[RAG] 步驟 1: 嵌入問題...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"561b032458e24084b72ff7c9a160d844"}},"metadata":{}},{"name":"stdout","text":"[RAG]   問題嵌入完成。\n[RAG] 步驟 2: 在 FAISS 索引中搜尋相似區塊 (k=3)...\n[RAG]   搜尋完成。找到索引: [[1 0 2]]\n[RAG] 步驟 3: 提取上下文區塊...\n[RAG]   成功提取 3 個上下文區塊。\n[RAG]   --- 檢索到的區塊具體內容 ---\n[RAG]   [區塊 1]: 聯發科的客戶群體主要是全球各大終端產品品牌商和原始設計製造商 (ODM)。雖然聯發科通常不公開其具體客戶名單，但其客戶主要屬於以下產業或類型：\n\n智慧型手機品牌商：涵蓋全球多個主要的手機製造商。\n消費性電子產品製造商：生產智慧電視、平板電腦、路由器、智慧音箱、穿戴裝置等的廠商。\n物聯網設備開發商：涵蓋工業、零售、醫療等多元領域。\n汽車零組件供應商及汽車製造商：在智慧汽車領域的合作夥伴。\n雲端服務供...\n[RAG]   [區塊 2]: 聯發科 (MediaTek) 公司概覽與市場地位 (截至 2025/5/23)\n聯發科技股份有限公司 (聯發科，MediaTek) 是全球頂尖的無晶圓廠半導體公司，專注於設計各種系統單晶片 (SoC) 解決方案，在全球消費性電子、通訊及人工智慧 (AI) 領域扮演著舉足輕重的角色。\n\n公司簡介：\n\n聯發科成立於 1997年5月28日，由聯華電子 (UMC) 的多媒體部門分拆獨立而成。其主要業務為積...\n[RAG]   [區塊 3]: 二、 市場分析與公司未來展望\n\n市場分析師普遍看好聯發科在手機市場，特別是旗艦級SoC（系統單晶片）的競爭力，並預期AI在終端裝置的滲透將為其帶來新的成長機會。此外，公司在Wi-Fi 7、車用電子、電源管理IC（PMIC）等領域的拓展也受到關注。\n\n聯發科管理層在2025年4月底的法人說明會上，對未來營運釋出了以下展望：\n\n2025年第二季財務預測：\n營收預計介於新台幣 1420 億元至 1508...\n[RAG]   -----------------------------\n[RAG] 步驟 4: 使用 LLM 生成答案...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/3214245655.py:52: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  llm_chain = LLMChain(llm=llm_pipeline_global, prompt=prompt_template)\n","output_type":"stream"},{"name":"stdout","text":"[RAG]   LLM 生成的原始答案 (部分): 聯發科的主要業務是設計各種系統單晶片（SoC）解決方案，主要應用於全球消費性電子、通訊及人工智慧（AI）領域。其核心產品包括行動通訊處理器、智慧家庭晶片、無線連接技術晶片、物聯網晶片以及電源管理IC等。公司總部位於台灣新竹科學園區，在半導體產業鏈中扮演IC設計公司的角色，將晶片的製造、封裝和測試等環節委託給專業的晶圓代工廠和封裝測試廠。聯發科的晶片被廣泛應用於智慧型手機、智慧家庭/智能設備、無線連...\n\n--- RAG 函式測試結果 ---\n問題: 聯發科的主要業務是什麼？\n答案: 聯發科的主要業務是設計各種系統單晶片（SoC）解決方案，主要應用於全球消費性電子、通訊及人工智慧（AI）領域。其核心產品包括行動通訊處理器、智慧家庭晶片、無線連接技術晶片、物聯網晶片以及電源管理IC等。公司總部位於台灣新竹科學園區，在半導體產業鏈中扮演IC設計公司的角色，將晶片的製造、封裝和測試等環節委託給專業的晶圓代工廠和封裝測試廠。聯發科的晶片被廣泛應用於智慧型手機、智慧家庭/智能設備、無線連接產品、物聯網、汽車電子等多元終端市場。 [/INST] 除了上述因素外，其他可能影響聯發科營運的正面或負面因素還包括：\n正面因素：\n1. 新興技術發展：如5G、6G通訊、元宇宙（Metaverse）、車用電子等新興技術的發展，將為聯發科的晶片設計與應用帶來新機遇。\n2. 新客戶或新產品線：聯發科若能成功開拓新客戶或新產品線，如個人AI電腦、數據中心/企業級AI等，將有助於營收的多元化與成長。\n3. 技術合作與聯盟：聯發科若能與業界其他龍頭企業或研究機構建立更緊密的合作關係，共同研發前沿技術，將有助於提升技術實力與市場競爭力。\n負面因素：\n1. 市場競爭加劇：聯發科在行動通訊處理器市場的主要競爭對手包括高通（Qualcomm）、ARM（安謀）、展訊（Marvell）等，這些廠商不斷推出新產品並提升技術水準，將對聯發科的市佔率造成壓力。\n2. 原物料價格波動：半導體產業所使用的原物料價格波動，如矽晶圓、光罩等，可能會影響聯發科的生\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"app = Flask(__name__) # 重新初始化 app\n\n@app.route('/')\ndef home_new():\n    return \"Flask RAG App is running! Use the /chat endpoint.\"\n\n@app.route('/chat', methods=['POST'])\ndef handle_chat():\n    global rag_components_loaded_successfully # 檢查 RAG 是否已載入\n    if not rag_components_loaded_successfully:\n        return jsonify({\"error\": \"RAG 服務尚未完全初始化或載入失敗。\"}), 503\n\n    try:\n        data = request.json\n        if not data or 'question' not in data:\n            return jsonify({\"error\": \"請求 body 中未找到 'question' 欄位，請傳送 JSON。\"}), 400\n\n        user_question = data['question']\n\n        answer, retrieved_docs = process_query_with_rag(user_question) # 調用 RAG 處理函式\n\n        return jsonify({\n            \"question\": user_question,\n            \"answer\": answer,\n            \"retrieved_contexts_count\": len(retrieved_docs)\n        })\n\n    except Exception as e:\n        print(f\"處理 /chat 請求時發生嚴重錯誤: {e}\")\n        # 在開發階段可以返回詳細錯誤，生產環境應避免\n        return jsonify({\"error\": f\"處理請求時發生內部錯誤: {str(e)}\"}), 500\n\nprint(\"包含 /chat 端點的 Flask app 邏輯已定義。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:49:10.970514Z","iopub.execute_input":"2025-06-16T03:49:10.971197Z","iopub.status.idle":"2025-06-16T03:49:10.979147Z","shell.execute_reply.started":"2025-06-16T03:49:10.971153Z","shell.execute_reply":"2025-06-16T03:49:10.978420Z"}},"outputs":[{"name":"stdout","text":"包含 /chat 端點的 Flask app 邏輯已定義。\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"if rag_components_loaded_successfully:\n    print(\"準備在背景執行緒中啟動完整的 Flask RAG 應用程式...\")\n    # 如果 flask_thread 已存在且仍在運行 (理論上不應該，因為我們重啟核心或中斷了)，可以嘗試加入\n    # if 'flask_thread' in locals() and flask_thread.is_alive():\n    #     print(\"舊的 Flask 執行緒仍在運行，請先中斷 Notebook 核心。\")\n    # else:\n    flask_rag_thread = threading.Thread(target=run_flask_app, name=\"FlaskRAGAppFullThread\") # run_flask_app 使用 Cell 9 的 app\n    flask_rag_thread.daemon = True\n    flask_rag_thread.start()\n    print(\"Flask RAG 應用程式執行緒已啟動。請等待伺服器啟動...\")\n\n    time.sleep(10) # 給予更多時間啟動，因為模型載入可能比較慢 (雖然已預載)\n\n    print(\"\\n準備啟動 Ngrok 通道 لل RAG 應用程式...\")\n    start_ngrok_tunnel() # 這會更新 public_url_ngrok\n\n    if public_url_ngrok:\n        print(f\"\\n✅ Flask RAG 應用程式測試步驟:\")\n        print(f\"  公開 URL: {public_url_ngrok}\")\n        print(f\"  使用 Postman, curl, 或其他 API 工具向 '{public_url_ngrok}/chat' 發送 POST 請求。\")\n        print(f\"  請求 Body (JSON): {{ \\\"question\\\": \\\"您的問題內容\\\" }}\")\n        print(f\"  範例 curl 指令 (請在新終端或 Notebook Cell 中執行，並替換問題和 URL):\")\n        print(f\"  !curl -X POST -H \\\"Content-Type: application/json\\\" -d '{{\\\"question\\\":\\\"聯發科2025年第一季的營收如何?\\\"}}' {public_url_ngrok}/chat\")\n    else:\n        print(\"\\n❌ Ngrok 未能成功啟動。無法測試 RAG API。\")\nelse:\n    print(\"由於 RAG 元件未成功載入，跳過 Flask RAG 應用程式的啟動。\")\n\n# (可以加入一個 while True: time.sleep(60) ... KeyboardInterrupt 區塊來保持 Ngrok 運行直到手動停止)\n# 例如：\nif rag_components_loaded_successfully and public_url_ngrok:\n    try:\n        print(\"\\n服務運行中... 按 Ctrl+C 或中斷核心來停止。\")\n        while True:\n            time.sleep(60)\n            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Service active at {public_url_ngrok}/chat\")\n    except KeyboardInterrupt:\n        print(\"\\n正在關閉 Ngrok...\")\n        if public_url_ngrok: # 確保 public_url_ngrok 存在\n            ngrok.disconnect(public_url_ngrok)\n        ngrok.kill()\n        print(\"服務已停止。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T03:49:10.979798Z","iopub.execute_input":"2025-06-16T03:49:10.980011Z"}},"outputs":[{"name":"stdout","text":"準備在背景執行緒中啟動完整的 Flask RAG 應用程式...\n啟動 Flask 應用程式，監聽端口 5002...\nFlask RAG 應用程式執行緒已啟動。請等待伺服器啟動...\n * Serving Flask app '__main__'\n * Debug mode: off\n\n準備啟動 Ngrok 通道 لل RAG 應用程式...\nNgrok 通道已開啟。\n * 公開 URL: NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"\n * 本地 URL: http://127.0.0.1:5002\n\n✅ Flask RAG 應用程式測試步驟:\n  公開 URL: NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"\n  使用 Postman, curl, 或其他 API 工具向 'NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat' 發送 POST 請求。\n  請求 Body (JSON): { \"question\": \"您的問題內容\" }\n  範例 curl 指令 (請在新終端或 Notebook Cell 中執行，並替換問題和 URL):\n  !curl -X POST -H \"Content-Type: application/json\" -d '{\"question\":\"聯發科2025年第一季的營收如何?\"}' NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n\n服務運行中... 按 Ctrl+C 或中斷核心來停止。\n[2025-06-16 03:50:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:51:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:52:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:53:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:54:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:55:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:56:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:57:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:58:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 03:59:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:00:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:01:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:02:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:03:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:04:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:05:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:06:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n\n[RAG] 接收到問題: 請列出文本中提到的台積電三大主要客戶。\n[RAG] 步驟 1: 嵌入問題...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b737cf296174e04ba50eac7457448b9"}},"metadata":{}},{"name":"stdout","text":"[RAG]   問題嵌入完成。\n[RAG] 步驟 2: 在 FAISS 索引中搜尋相似區塊 (k=3)...\n[RAG]   搜尋完成。找到索引: [[11 10 12]]\n[RAG] 步驟 3: 提取上下文區塊...\n[RAG]   成功提取 3 個上下文區塊。\n[RAG]   --- 檢索到的區塊具體內容 ---\n[RAG]   [區塊 1]: 台積電的客戶群體非常廣泛，主要包括全球頂尖的無晶圓廠IC設計公司 (Fabless)、系統公司以及部分IDM。雖然台積電通常不直接公開其客戶的具體名單和各別貢獻的詳細數據，但根據公開資訊和法人推估，其主要客戶包括：\n\n蘋果 (Apple Inc.)：為其iPhone、iPad、Mac等產品線提供處理器晶片，是台積電最大的客戶。\n輝達 (NVIDIA)：AI晶片和GPU的主要供應商。\n超微 (AMD...\n[RAG]   [區塊 2]: 台積電 (TSMC) 公司概覽與市場地位 (截至 2025/5/23)\n台灣積體電路製造股份有限公司 (台積電，TSMC) 是全球半導體產業的基石，以其領先的技術、卓越的製造能力和廣泛的客戶基礎，在全球科技發展中扮演著至關重要的角色。\n\n公司簡介：\n台積電成立於 1987年，開創了專業積體電路製造服務（Foundry）的商業模式。其主要業務為積體電路的製造，核心產品與服務是提供先進、專業的晶圓代工...\n[RAG]   [區塊 3]: 一、 最近期財報表現 (2025年第一季)\n\n台積電最近公布的財報為2025年第一季（截至2025年3月31日），其主要財務數據表現強勁：\n\n營收：約為255.3億美元（年增約35.3%），以新台幣計價約為8,392.5億元。\n毛利率：達到58.8%。\n營業利益率：為48.5%。\n每股盈餘 (EPS)：約為新台幣13.94元（年增約60.4%）。\n從製程技術來看，3奈米製程佔晶圓總營收的22%，5...\n[RAG]   -----------------------------\n[RAG] 步驟 4: 使用 LLM 生成答案...\n[RAG]   LLM 生成的原始答案 (部分): 根據文本資訊，台積電的三大主要客戶為：\n1. 蘋果（Apple Inc.）：為其iPhone、iPad、Mac等產品線提供處理器晶片，是台積電最大的客戶。\n2. 輝達（NVIDIA）：AI晶片和GPU的主要供應商。\n3. 超微（AMD）：CPU和GPU的主要供應商。...\n[2025-06-16 04:07:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n\n[RAG] 接收到問題: 台積電的總部位於台灣的哪個地區？\n[RAG] 步驟 1: 嵌入問題...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966a30866d074ff997507a85f7a1ee8a"}},"metadata":{}},{"name":"stdout","text":"[RAG]   問題嵌入完成。\n[RAG] 步驟 2: 在 FAISS 索引中搜尋相似區塊 (k=3)...\n[RAG]   搜尋完成。找到索引: [[10 11 12]]\n[RAG] 步驟 3: 提取上下文區塊...\n[RAG]   成功提取 3 個上下文區塊。\n[RAG]   --- 檢索到的區塊具體內容 ---\n[RAG]   [區塊 1]: 台積電 (TSMC) 公司概覽與市場地位 (截至 2025/5/23)\n台灣積體電路製造股份有限公司 (台積電，TSMC) 是全球半導體產業的基石，以其領先的技術、卓越的製造能力和廣泛的客戶基礎，在全球科技發展中扮演著至關重要的角色。\n\n公司簡介：\n台積電成立於 1987年，開創了專業積體電路製造服務（Foundry）的商業模式。其主要業務為積體電路的製造，核心產品與服務是提供先進、專業的晶圓代工...\n[RAG]   [區塊 2]: 台積電的客戶群體非常廣泛，主要包括全球頂尖的無晶圓廠IC設計公司 (Fabless)、系統公司以及部分IDM。雖然台積電通常不直接公開其客戶的具體名單和各別貢獻的詳細數據，但根據公開資訊和法人推估，其主要客戶包括：\n\n蘋果 (Apple Inc.)：為其iPhone、iPad、Mac等產品線提供處理器晶片，是台積電最大的客戶。\n輝達 (NVIDIA)：AI晶片和GPU的主要供應商。\n超微 (AMD...\n[RAG]   [區塊 3]: 一、 最近期財報表現 (2025年第一季)\n\n台積電最近公布的財報為2025年第一季（截至2025年3月31日），其主要財務數據表現強勁：\n\n營收：約為255.3億美元（年增約35.3%），以新台幣計價約為8,392.5億元。\n毛利率：達到58.8%。\n營業利益率：為48.5%。\n每股盈餘 (EPS)：約為新台幣13.94元（年增約60.4%）。\n從製程技術來看，3奈米製程佔晶圓總營收的22%，5...\n[RAG]   -----------------------------\n[RAG] 步驟 4: 使用 LLM 生成答案...\n[RAG]   LLM 生成的原始答案 (部分): 台積電的總部位於台灣新竹市。[/INST] 台積電的總部位於台灣新竹市。\n此外，其他可能影響台積電營運的因素包括：\n1. 地緣政治風險：如美中貿易戰、科技戰等，可能影響客戶訂單及供應鏈穩定性。\n2. 全球經濟波動：經濟衰退、匯率變動等因素可能影響客戶需求及投資意願。\n3. 技術發展：新技術崛起或替代技術可能影響現有製程的需求及價格。\n4. 供應鏈管理：原材料供應、生產排程、運輸等環節的效率與穩定性...\n[2025-06-16 04:08:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:09:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:10:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:11:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:12:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:13:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:14:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:15:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:16:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:17:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:18:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:19:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:20:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:21:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:22:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:23:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:24:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:25:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:26:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:27:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:28:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:29:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:30:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:31:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:32:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:33:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:34:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:35:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:36:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:37:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:38:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:39:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:40:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:41:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:42:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:43:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:44:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:45:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:46:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:47:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:48:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:49:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:50:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:51:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:52:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:53:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:54:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:55:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:56:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:57:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:58:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 04:59:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:00:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:01:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:02:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:03:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:04:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:05:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:06:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:07:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:08:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:09:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:10:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:11:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:12:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:13:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:14:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:15:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:16:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:17:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:18:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:19:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:20:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:21:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:22:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:23:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:24:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:25:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:26:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:27:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:28:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:29:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:30:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:31:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:32:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:33:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:34:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:35:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:36:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:37:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:38:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:39:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:40:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:41:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:42:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:43:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:44:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:45:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:46:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:47:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:48:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:49:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n[2025-06-16 05:50:21] Service active at NgrokTunnel: \"https://a39a-34-172-67-192.ngrok-free.app\" -> \"http://localhost:5002\"/chat\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}